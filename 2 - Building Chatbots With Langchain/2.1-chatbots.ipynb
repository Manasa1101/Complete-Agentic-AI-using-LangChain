{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0a4213c",
   "metadata": {},
   "source": [
    "### Building a Chatbot \n",
    "\n",
    "In this video we'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions \n",
    "\n",
    "Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n",
    "\n",
    "- Conversational RAG: Enable a chatbot experience over an external source of data\n",
    "- Agents: Build a chatbt that can take actions\n",
    "\n",
    "This video tutorial will cover the basics which will be helpful for those two more advanced topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326914f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d60223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() ## loading all the environment variables\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ab6323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001CD3196B5E0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001CD319A9A20>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b6e3757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Krish! It's nice to meet you.\\n\\nBeing a Chief AI Engineer is a fascinating role. What kinds of projects are you working on these days?  \\n\\nI'm always eager to learn more about the work being done in the field of AI.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 22, 'total_tokens': 79, 'completion_time': 0.103636364, 'prompt_time': 0.001325749, 'queue_time': 0.254724086, 'total_time': 0.104962113}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4f6e8cfe-855d-427b-82e8-47634c9a30b3-0', usage_metadata={'input_tokens': 22, 'output_tokens': 57, 'total_tokens': 79})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi, My name is Krish and I am a chief AI engineer\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00ede418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Krish, and you are a Chief AI Engineer!  üòä \\n\\nIs there anything else you'd like me to remember about you? \\n\\nPerhaps your favorite AI model or a project you're particularly proud of?  I'm here to learn more about you.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 99, 'total_tokens': 160, 'completion_time': 0.110909091, 'prompt_time': 0.003049067, 'queue_time': 0.257661203, 'total_time': 0.113958158}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--895a77db-73f1-4134-81ad-6e84e83fc637-0', usage_metadata={'input_tokens': 99, 'output_tokens': 61, 'total_tokens': 160})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"HI, My name is Krish and I am a chief AI Engineer\"),\n",
    "        AIMessage(content=\"Hello Krish! It's nice to meet you.\\n\\nBeing a Chief AI Engineer is a fascinating role. What kinds of projects are you working on these days?  \\n\\nI'm always eager to learn more about the work being done in the field of AI.\\n\"),\n",
    "        HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
    "    \n",
    "    ]   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12db66",
   "metadata": {},
   "source": [
    "So one important thing is that whateven conversation I'm giving inside this list of messages with the human messsage, I message human message, It is also able to remember the previous context. \n",
    "\n",
    "Okay, now that we are going to discuss about message history, the message history will probably help you to understand all the context that help the LLM to remember( all the context that we are discussing about). This is just an idea whether LLM is able to remember things or not.\n",
    "\n",
    "But at the end of the day whereever we develop real world application, there will be different sessions that will be happenning, right? \n",
    "Let's say I am using chatgpt, some other is using chatgpt, some other people are using other LLM models and they are chatting, right?\n",
    "\n",
    "How that particular models are able to remember their context, that is with respect to session?\n",
    "\n",
    "So in oreder to understand that how these sessions are managed, we will be discussing about a VERY IMPORTANT PROPERTY WHICH IS CALLED A MESSAGE HISTORY."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2cf9a",
   "metadata": {},
   "source": [
    "### Message History\n",
    "\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will use then load these messsages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "328b48c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_community) (0.3.69)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_community) (0.3.26)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_community) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_community) (3.12.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_community) (2.10.0)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_community) (0.4.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_community) (2.2.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (2.11.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (4.14.0)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2025.6.15)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5fff70",
   "metadata": {},
   "source": [
    "Any integration with respect to message history will be available in this langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7e177db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "# ChatMessageHistory is an im-memory implementation of chat message history. Stores messages in an memory list.\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "# BaseChatMessageHistory is an abstract class for storing chat message history.\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "# Runnable that manages chat message history for other runnable. A chat message history is a sequence of message that represent a conversation\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store: \n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "'''Whenever I give a session ID it should first of all go ahead and check in this particular dictionary\n",
    "\n",
    "whether it is available or not.\n",
    "\n",
    "If it is available, it will go ahead and pick up the entire chat message that we have discussed with\n",
    "\n",
    "respect to or whatever questions we have asked with respect to the model for that session ID, and it\n",
    "\n",
    "is just going to return it'''\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e1483",
   "metadata": {},
   "source": [
    "We can import the relevant class here, set up our chain which wraps our model and add in the message history.\n",
    "\n",
    "But the thing that you really need to understand is that whenever different, different users are chatting\n",
    "\n",
    "with the LM model, how we are going to make sure that one session is completely different from the\n",
    "\n",
    "other session.\n",
    "\n",
    "So for that reason, what we will do is that we will create one amazing function, okay.\n",
    "\n",
    "And this particular function will be definition get_session_history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8cb1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4a9cc9",
   "metadata": {},
   "source": [
    "First of all we have created a dictionary over here.\n",
    "\n",
    "Then we have created a function which will be able to retrieve whatever, um, chat history is there\n",
    "\n",
    "for a specific session ID then we are converting or we are using this runnable with message history\n",
    "\n",
    "so that we can interact with our LM model based on the chat history.\n",
    "\n",
    "So for this we give two parameters that is model and get session history.\n",
    "\n",
    "Now we have created a configuration called as session ID as chat one.\n",
    "\n",
    "And now what we will do is that we will call this with message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c12ad892",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"HI, My name is Krish and I am a chief AI Engineer\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad148de8",
   "metadata": {},
   "source": [
    "Now when we are gave the config that basically means for this session id we are interacting okay.\n",
    "\n",
    "So based on the session id it will be able to remember all this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a29c0ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Krish!  \\n\\nIt's great to connect with another AI professional. What kind of AI projects are you most passionate about?  Do you have a favorite area of focus within the field? \\n\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cffe8f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Krish.  You told me at the beginning of our conversation! üòä \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 314, 'total_tokens': 335, 'completion_time': 0.038181818, 'prompt_time': 0.006746001, 'queue_time': 0.252969194, 'total_time': 0.044927819}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--e130be56-5e74-4b90-87b1-78b36edc651c-0', usage_metadata={'input_tokens': 314, 'output_tokens': 21, 'total_tokens': 335})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"HI, What is my name?\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9456044b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me your name, I'd be happy to use it! üòä\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Change the congig --> session id\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name?\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e5efad",
   "metadata": {},
   "source": [
    "Now what exactly I have actually done over here. I have just changed the session ID. Now, whatever conversation I have right now, it will entirely be saved in the session ID chat two.\n",
    "\n",
    "So that in the later stages, whenever we use this with message history, it will just give the session ID of chat two and it will retrieve the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8f7ed91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi John, it's nice to meet you! üëã \\n\\nHow can I help you today? üòä  \\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My name is John\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3620886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is John, as you told me earlier. üòä \\n\\nIs there anything else I can help you with, John?  \\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name?\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a7646",
   "metadata": {},
   "source": [
    "# Prompt templates\n",
    "\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In case, the raw user input is just a message, which we are passing to the LLM, Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions ( but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2af3d751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Answer all the question to the best of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt|model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161274ac",
   "metadata": {},
   "source": [
    "Whatever human message we specifically give, it needs to be given in a key value pairs where the key name should be messages.\n",
    "\n",
    "And that is the reason we are giving a message placeholder over here. Right before we were not using any message placeholder.\n",
    "\n",
    "So over there we were giving in the form of list of messages like human message, AI message, system message like this.\n",
    "\n",
    "Right.\n",
    "\n",
    "But here I've told that, hey, we are going to use a message placeholder called as variable name as messages.\n",
    "\n",
    "And inside this whatever information you definitely require from the human side, it will be given in that.\n",
    "\n",
    "So how do I call it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b49d39ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Krish! \\n\\nIt's nice to meet you.  \\n\\nHow can I help you today? üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 30, 'total_tokens': 58, 'completion_time': 0.050909091, 'prompt_time': 0.001404341, 'queue_time': 0.253795739, 'total_time': 0.052313432}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--75231394-aaf7-4d8e-9b52-2e8ff6e17a69-0', usage_metadata={'input_tokens': 30, 'output_tokens': 28, 'total_tokens': 58})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Krish\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bed13ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3750e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Krishna, it's nice to meet you! üëã \\n\\nIs there anything I can help you with today? üòÑ  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 80, 'total_tokens': 109, 'completion_time': 0.052727273, 'prompt_time': 0.0024344, 'queue_time': 0.25449461, 'total_time': 0.055161673}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--e9587931-8d6c-49f9-9315-c4680298cd98-0', usage_metadata={'input_tokens': 80, 'output_tokens': 29, 'total_tokens': 109})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\":\"chat3\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is Krishna\")],\n",
    "    config=config\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b627fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add more complexity \n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "478737c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§ï‡•É‡§∑! \\n\\n‡§Æ‡•Å‡§ù‡•á ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•ã‡§ó‡•Ä‡•§ ‡§Ü‡§™ ‡§ú‡•ã ‡§≠‡•Ä ‡§™‡•Ç‡§õ‡§®‡§æ ‡§ö‡§æ‡§π‡•á‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡§æ ‡§∏‡§∞‡•ç‡§µ‡§∂‡•ç‡§∞‡•á‡§∑‡•ç‡§† ‡§™‡•ç‡§∞‡§Ø‡§æ‡§∏ ‡§ï‡§∞‡•Ç‡§Å‡§ó‡§æ‡•§  üòä\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Krish\")],\"language\":\"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d3c160",
   "metadata": {},
   "source": [
    "Now there are two keys that are going over here.\n",
    "\n",
    "So with respect to the chat message history how it is going to change let's see"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef80402",
   "metadata": {},
   "source": [
    "Let's wrap this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we specify the correct key to save the chat history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a24891",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
