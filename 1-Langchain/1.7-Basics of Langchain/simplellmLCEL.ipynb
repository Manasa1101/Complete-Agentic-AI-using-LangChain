{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c5c568",
   "metadata": {},
   "source": [
    "### Build a Simple LLM Application with LCEL\n",
    "\n",
    "In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simle LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with langchain - a lot of features can be built with just some prompting and an LLM call!\n",
    "\n",
    "After seeing this video, you'll have a high level overview of: \n",
    "\n",
    "- Using language models\n",
    "- Using PromptTemplates and OuputParsers\n",
    "- Using Langchain Expression Language (LCEL) to chain components together\n",
    "- Debugging and tracing your application using LangSmith\n",
    "- Deploying your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e2013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Open AI API key and Open Source models -- Llama3, Gemma2, mistram-Groq\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import openai\n",
    "openai.api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "groq_api_key=os.getenv(\"GROK_API_KEY\")\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64150e",
   "metadata": {},
   "source": [
    "Grok is the AI infrastructure company that delivers fast AI inference. It has deployed many models in its own cloud and over there they have used this amazing inferencing known as LPU AI inferencing engine. \n",
    "\n",
    "An LPU inferencing Engine, with LPU standing for Language Processing unit is a hardware and software platform that delivers exceptional compute speed, quality, and energy efficiency. This new type of end - to - end processing unit system provides the fastest inference for computationally intensive applications with sequencial compnents, such as AI language applications like LLM's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3f96cd",
   "metadata": {},
   "source": [
    "The LPU is designed the two LLM bottlenecks, compute density and memory bandwidth. An LPU has greater compute capacity than a GPU and CPU in regards to LLMs. This reduces the amount of time per word calculated, allowing sequences of text to be generated much faster. Additionally, eliminating external memory bottlenecks enables the LPU Inference Engine to deliver orders of magnitude better performance on LLMs compared to GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b0a873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_groq in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_groq) (0.3.69)\n",
      "Requirement already satisfied: groq<1,>=0.29.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_groq) (0.30.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from groq<1,>=0.29.0->langchain_groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from groq<1,>=0.29.0->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from groq<1,>=0.29.0->langchain_groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from groq<1,>=0.29.0->langchain_groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from groq<1,>=0.29.0->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from groq<1,>=0.29.0->langchain_groq) (4.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.29.0->langchain_groq) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.29.0->langchain_groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.29.0->langchain_groq) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.29.0->langchain_groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.29.0->langchain_groq) (0.16.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain_groq) (0.4.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain_groq) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain_groq) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain_groq) (6.0.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain_groq) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain_groq) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain_groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain_groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain_groq) (0.4.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_groq) (3.10.18)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_groq) (2.32.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_groq) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_groq) (0.23.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_groq) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_groq) (2.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ecd081",
   "metadata": {},
   "source": [
    "This is what langchain is basically doing. Langchain wants integration with every platform out there, every LLM models out there. They will say, hey you all bigger companies, you keep on competing, you bring up new new models, but we will create a wrapper which will help us to integrate any kind of model that comes into the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44598161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001C14040B5E0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C140409150>, model_name='gemma2-9b-it', model_kwargs={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"gemma2-9b-it\",groq_api_key=groq_api_key)\n",
    "model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b35bc",
   "metadata": {},
   "source": [
    "Building an LLM application basically means let's say if I'm giving any query to my model, my model should be able to give some response. But you should understand some very key libraries that are specifically used in langchain which we also say it as Runnables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c43f4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_core in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (0.3.69)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_core) (0.4.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_core) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_core) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_core) (4.14.0)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_core) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langchain_core) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain_core) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain_core) (3.10.18)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain_core) (2.32.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain_core) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain_core) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain_core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain_core) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain_core) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain_core) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain_core) (2.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\prasa\\onedrive\\desktop\\langchain\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb773881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "# Whenever we chat or whenever we  provide any messages to our LLM model, we should specify our LLM thst which message is basically provided by the human being, and which message is a kind of instruction to the LLM model.\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content= \"Translate the following from English to French\"),\n",
    "    HumanMessage(content=\"Hello How are you?\")\n",
    "]\n",
    "result = model.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "502de1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here are a few ways to say \"Hello, how are you?\" in French:\\n\\n**Formal:**\\n\\n* **Bonjour, comment allez-vous ?** (This is the most formal option and is used with people you don\\'t know well, people in authority, or in professional settings.)\\n\\n**Informal:**\\n\\n* **Salut, comment vas-tu ?** (This is used with friends and family.)\\n\\n* **Coucou, ça va ?** (This is a very informal and friendly way to say hello.) \\n\\n\\nLet me know if you\\'d like to learn more French greetings!\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 125, 'prompt_tokens': 21, 'total_tokens': 146, 'completion_time': 0.227272727, 'prompt_time': 0.001354339, 'queue_time': 0.274117741, 'total_time': 0.228627066}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--af41963f-1151-4560-a75f-0eb4f9152f81-0', usage_metadata={'input_tokens': 21, 'output_tokens': 125, 'total_tokens': 146})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f00dc9",
   "metadata": {},
   "source": [
    "The above result is nothing but a AI message, but I want to only retrieve this particular output. So for that in langchain_core we have something called output_parsers. The Output parser will be responsible to basically display the message that is coming outside as a response from the LLM model. We can create custom string output parser if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d316a68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are a few ways to say \"Hello, how are you?\" in French:\\n\\n**Formal:**\\n\\n* **Bonjour, comment allez-vous ?** (This is the most formal option and is used with people you don\\'t know well, people in authority, or in professional settings.)\\n\\n**Informal:**\\n\\n* **Salut, comment vas-tu ?** (This is used with friends and family.)\\n\\n* **Coucou, ça va ?** (This is a very informal and friendly way to say hello.) \\n\\n\\nLet me know if you\\'d like to learn more French greetings!\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b0971f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello - Bonjour \\n\\nHow are you? - Comment allez-vous ? (formal) or Ça va ? (informal) \\n\\n\\nLet me know if you'd like to practice more translations! \\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Using LCEL we can chain the components\n",
    "chain = model|parser\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d9bc7",
   "metadata": {},
   "source": [
    "Right now we are passing everytime list of messages. Instead of this we can use one more efficient technique. And it is called as prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a074101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Templates\n",
    "# So instead of just giving list of message, I will try to take a combination of user input and some application logic. Where I am able to give the instruction. I am able to give the userinput over ther. \n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "# First we should define the generic template\n",
    "generic_template=\"Translate the following into {language}:\"\n",
    "\n",
    "# After generic template we should create prompt\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [(\"system\",generic_template),(\"user\",\"{text}\")]\n",
    "    # this text we should give in run time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69d8cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=prompt.invoke({\"language\":\"French\",\"text\":\"Hello\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39ce75a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into French:', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ad2b6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour \\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=prompt|model|parser\n",
    "chain.invoke({\"language\":\"French\",\"text\":\"Hello\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686ba45",
   "metadata": {},
   "source": [
    "# Langserve helps developers deploy LangChain runnables and chains as REST API.\n",
    "\n",
    "# This Library is integrated with FAST API and uses pydantic for data validation.\n",
    "\n",
    "# In addition, it provides a client that can be used to call into runnables deployed on a server. A javascript client is available in LangChain.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6237f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
