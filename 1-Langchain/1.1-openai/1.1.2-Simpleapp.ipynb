{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a25f0a63",
   "metadata": {},
   "source": [
    "### Simple Gen AI app using Lnagchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ##using to set up environmental variable\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\" ## This is required for langchain\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd3dea30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Import beautifulsoup4 library:- It helps to scrap the entire website data\n",
    "\n",
    "## Data Ingestion -- From the website we need to scrap the data\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c0c903",
   "metadata": {},
   "source": [
    "# WebBaseLoader is something kind of library where what ever link you specifically get or give, it will be able to scrap the entire content from that particular website\n",
    "\n",
    "pre requisite is we should have beautifulsoup4 library because internally webbaseloader uses beautifulsoup4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1940ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"Website Link\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e036c05",
   "metadata": {},
   "source": [
    "# Every RAG application performs this thing. First of all we read our entire data source from a specific data source itself. Then we will load this and convert this into our documents. Once we get that document. It is a huge document right. Now in this particular case if it is a huge document, we need to divide this entire document into chunks. You cannot directly give this entire document to our LLM Model. The reason is very simpl with respect to every LLM model, there is some context size. There is a limitation with respect to context size. And as we go ahead and see different different models that are goin to come up, the context size will keep on increasing. But it is always a good idea that we divide this entire document into chunks of text. After we divide all that data into chunks, we convert this into vectors by using some king of vector embedding( Vector embedding are some techniques where  we are able to convert all this text into vectors). After we get all the vector embeddings we try to store it in a vector store DB. So these are he steps.\n",
    "\n",
    "# Load Data --> Docs --> Divide our text into chunks --> text --> vectors --> Vector Embeddings --> Vector Store DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5c38b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have to split our entire documents into chunks of text we will use lanchain_text_splitters\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# In this we also have an option to specify our chunk size. And I can also overlap my text while doing this splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "\n",
    "# Now we will use this textsplitter and we are going to split all the document in to multiple documents\n",
    "documents = text_splitter.splti_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting text chunks to vectors. The reason we work with Q and A chatbot or document Q&A chatbot, let's consider Rag application, a very simple algorithm is basically used name \"COSINE SIMILARITY\". And based on the cosine similarity usually gets applied in the vectors itself. \n",
    "# Embedding Techniques\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "#OpenAIEmbeddings is a very efficient embedding technique\n",
    "embeddings= OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2461dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to store these vectors into some kind of vector database\n",
    "# Faiss vector database is created by facebook and you can use it very much efficiently and there also similarity search is applied in the backend\n",
    "# import faiss-cpu for this\n",
    "\n",
    "import langchain_community.vectorstores import FAISS\n",
    "vectorstoredb = FAISS.from_documents(documents,embeddings) # First the documents get embedded into vectors and then get stored in db\n",
    "# finally you will be able to see that you are having this vector stored db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcfe405",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstoredb\n",
    "# you will also be able to save this in your local  environment or in harddisk whereever you want.\n",
    "# Now based on this vector store db we can query anything an get the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c882d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query from a vector store db \n",
    "\n",
    "query=\"Langsmith has two usage limits: total traces and extended\"\n",
    "result = vectorstoredb.similarity_search(query)\n",
    "\n",
    "# Once I get the result, I can go ahead and write result of zero and inside that we will be having a key which is called page content.\n",
    "result[0].page_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66e8ef",
   "metadata": {},
   "source": [
    "# Querying from vector store db will just try to give you based in the context, all the information that is available near those vectors. It will just come and display in front of you. But, let's say I want to ask a question which is much more meaningful. I really need to provide context with respect to that particular question. So that is the reason why we will be using retrieval chain. This is a very important step because whereever we will specifically be working, we have to use this retriever. We have to use this chain in most of that Rag applications or document Q&A chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm - ChatOpenAI(model=\"gpt-4.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f918a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval Chain, Document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_ocuments_chain #Create a chain for passing a list of documents to the model(helps in providing the context to the model increasing the spead of search).\n",
    "# MyLLM model should also have a contect about any documents to answer it muh more properly.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from this chatprompttemplate I am going to give my own custom prompt\n",
    "\n",
    "prompt= ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based on the provided context::\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "#Internally I have created a key on the particular name \"context\". This context is the information I will be giving to my LLM model regarding the information about the documents or text\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "document_chain\n",
    "\n",
    "# document_chain is a runnable binding and the mapper is somethin like this \n",
    "# first chat prompt template, then chat open AI then string output parser. All has been combined together in the form of a chain. After going through all this chain it should be able to give me some context information about the thing I am searching for right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3aa239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"Langsmith has two usage limits: total traces and extended traces\" # This is the input which the user is basically giving \n",
    "    \"context\":[Document(page_content=\"Langsmith has two usage limits: total traces and extended\")]# this is the context which I really need to give\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8b7539",
   "metadata": {},
   "source": [
    "However we want the documents to first come from the retriever we just set up. That way, we can use the retriever to dynamically select the most rlevant documents and pass those in for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d299131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retriever \n",
    "\n",
    "vectorstoredb ## This is having all the vectors information available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b34c8",
   "metadata": {},
   "source": [
    "Retriever can be considered as an interface and it's responsibiity is that if anybody asks any input, then this interface will just be a way of probably getting the data from the vector store DB where we don't even need to do the similarity search. So after we create this particular vector store db, we convet tis vector store db into a retriever. Retriever with respect to any input, It will be able to pass that particular input through this retriever and get the response from the vector store db. You can onsider this as a pathway to probably get the information from vector store db. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed607b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retriever_chain\n",
    "retrieval_chain=create_retriever_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed9392",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae8c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get  the response from the LLM\n",
    "\n",
    "response=retrival_chain.invoke({}\"input\":\"Langsmith has two usage limits: total traces and extended})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c4aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ffcb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "respnse['context']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
