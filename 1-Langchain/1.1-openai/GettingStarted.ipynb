{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e495d8",
   "metadata": {},
   "source": [
    "### Getting started with Langchain and Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: promp templates, models, and output parses. \n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b735de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ##using to set up environmental variable\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\" ## This is required for langchain\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f98544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x00000248D8F78F10> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000248DCE05750> root_client=<openai.OpenAI object at 0x00000248D8F78DC0> root_async_client=<openai.AsyncOpenAI object at 0x00000248DCE056C0> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c737f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and get response from LLM \n",
    "\n",
    "result = llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f48815",
   "metadata": {},
   "source": [
    "###  Prompt Template is like how you want your model to behave or what kind of response or what kind of role you really want to give your llm model. Simillarly langchain also supports different kinds of chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64fdf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ \n",
    "        #I am assigning role to my system(LLM Model)\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the question\")\n",
    "        # Users will provide you input. That input is like a placeholder. Like user is asking any question, then the system is basically AI engineer. The LLM model is bascally an AI engineer and it should provide me thee answer based on the questions provided by the users\n",
    "        (\"users\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a128e",
   "metadata": {},
   "source": [
    "## Chain basically means you can collaborate multiple things\n",
    "\n",
    "Like prompt, you can combine LLM, you creat your own custom prompt. And it just gives you an idea like how once input is given by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt|llm\n",
    "\n",
    "response = chain.invoke(\"input\":\"Can you tell me about Langsmith\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a67d178",
   "metadata": {},
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fe656e",
   "metadata": {},
   "source": [
    "## Output Parser is nothing but gettng the message from the llm and how you really want to display it. By default there is a string output parser class but you can also create yout own custom output parser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf35c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## String Output Parser\n",
    "#stroutput Parser\n",
    "\n",
    "from langchain_community.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser() ## initializing it.\n",
    "chain = prompt|llm|output_parser\n",
    "\n",
    "response = chain.involke({'input':\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d907c4a8",
   "metadata": {},
   "source": [
    "# This time you will be able to see the output parser. We ae able to get directly. we don't have any content. By string output parser, what ever message you are specifically goettin that is only going to get displayed. So it is very handy to used the strin output parser because based on your likes and dislikes you can actually display the output result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9089b91",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
